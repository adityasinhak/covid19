golem::fill_desc(
pkg_name = "coronavirus", # The Name of the package containing the App
pkg_title = "Tracking the Coronavirus", # The Title of the package containing the App
pkg_description = "Tracking the Coronavirus.", # The Description of the package containing the App
author_first_name = "Aditya", # Your First Name
author_last_name = "Sinha",  # Your Last Name
author_email = "aksrdx4u,2gmail.com",      # Your Email
repo_url = "https.//github/adityasinhak/covid19" # The (optional) URL of the GitHub Repo
)
# Set options here
options(golem.app.prod = FALSE) # TRUE = production mode, FALSE = development mode
TRUE
# Detach all loaded packages and clean your environment
golem::detach_all_attached()
# Document and reload your package
golem::document_and_reload()
# Add helper functions
golem::use_utils_ui()
golem::use_utils_server()
library("askpass", lib.loc="~/Library/R/3.4/library")
install.packages("golem")
detach("package:askpass", unload=TRUE)
install.packages("golem")
install.packages("gh")
install.packages("cli")
install.packages("cli")
install.packages("golem")
install.packages("usethis")
install.packages("fs")
install.packages("fs")
install.packages("golem")
# You're now set!
# go to dev/02_dev.R
rstudioapi::navigateToFile( "dev/02_dev.R" )
## 2.1 Add modules
##
golem::add_module( name = "trend" ) # Name of the module
golem::add_module( name = "count" ) # Name of the module
# Run the application
coronavirus::run_app()
golem::add_module( name = "map" ) # Name of the module
# Set options here
options(golem.app.prod = FALSE) # TRUE = production mode, FALSE = development mode
# Detach all loaded packages and clean your environment
golem::detach_all_attached()
# Document and reload your package
golem::document_and_reload()
#' Crawl Data
#'
#' Crawl John Hopkin's data and store in database.
#' Every time the function is called the entirety of
#' the data is pulled and the content of the database
#' is overwritten.
#'
#' @export
crawl_coronavirus <- function(){
con <- NULL
if(file.exists(config_file)){
con <- connect()
}
on.exit({
disconnect(con)
})
# get data
cli::cli_alert_info("Crawling data from John Hopkins")
# read data
suppressMessages({
confirmed <- readr::read_csv(confirmed_sheet, col_types = readr::cols())
#recovered <- readr::read_csv(recovered_sheet, col_types = readr::cols())
deaths <- readr::read_csv(deaths_sheet, col_types = readr::cols())
})
# add col
confirmed$type <- "confirmed"
#recovered$type <- "recovered"
deaths$type <- "death"
# rename
confirmed <- rename_sheets(confirmed)
#recovered <- rename_sheets(recovered)
deaths <- rename_sheets(deaths)
# pivot longer
confirmed <- pivot(confirmed)
#recovered <- pivot(recovered)
deaths <- pivot(deaths)
df <- dplyr::bind_rows(confirmed, deaths) %>%
dplyr::mutate(
date = as.Date(date, format = "%m/%d/%y"),
cases = trimws(cases),
cases = as.numeric(cases),
cases = dplyr::case_when(
is.na(cases) ~ 0,
TRUE ~ cases
),
country = dplyr::case_when(
country == "US" ~ "United States of America",
TRUE ~ country
),
country_iso2c = suppressWarnings(countrycode::countrycode(country, "country.name", "iso2c"))
)
cli::cli_alert_info("Crawling data from Weixin")
china <- nCov2019::get_nCov2019(lang = "zh")
china_daily <- china$chinaDayList %>%
dplyr::mutate(
date = paste0("2020.", date),
date = as.Date(date, "%Y.%m.%d")
) %>%
dplyr::mutate_if(is.character, as.numeric)
china_total <- as.data.frame(china$chinaTotal, stringsAsFactors = FALSE)
cli::cli_alert_info("Crawling data from DXY")
dxy_list <- xml2::read_html(dxy_url) %>%
rvest::html_node("#getAreaStat") %>%
rvest::html_text() %>%
gsub("try \\{ window.getAreaStat = ", "", .) %>%
gsub("\\}catch\\(e\\)\\{\\}", "", .) %>%
jsonlite::fromJSON()
dxy <- dxy_list %>%
dplyr::pull(cities) %>%
purrr::map2(dxy_list$provinceShortName, function(city, province){
if(nrow(city))
city$province <- province
return(city)
}) %>%
purrr::map2(dxy_list$provinceName, function(city, province){
if(nrow(city))
city$province_long <- province
return(city)
}) %>%
purrr::map_dfr(tibble::as_tibble) %>%
dplyr::left_join(chinese_provinces, by = c("province" = "chinese")) %>%
dplyr::rename(province_pinyin = state)
log <- tibble::tibble(last_updated = Sys.time())
# crawl news
news <- NULL
if(file.exists(config_file)){
if(has_newsapi()){
cli::cli_alert_info("Crawling news from newsapi.org")
set_news_api_token()
news <- newsapi::every_news("coronavirus OR covid", results = 100, language = "en", sort = "popularity")
} else {
cli::cli_alert_danger("Not `newsapi` entry in config file, not crawling news.")
}
}
# save
if(file.exists(config_file)){
cli::cli_alert_success("Writing to database")
DBI::dbWriteTable(con, "jhu", df, overwrite = TRUE, append = FALSE)
DBI::dbWriteTable(con, "weixin_total", china_total, overwrite = TRUE, append = FALSE)
DBI::dbWriteTable(con, "weixin", china_daily, overwrite = TRUE, append = FALSE)
DBI::dbWriteTable(con, "dxy", dxy, overwrite = TRUE, append = FALSE)
DBI::dbWriteTable(con, "log", log, append = TRUE)
DBI::dbWriteTable(con, "news", news, overwrite = TRUE)
}
dat <- list(
jhu = df,
weixin = china_daily,
weixin_total = china_total,
dxy = dxy,
news = news
)
invisible(dat)
}
View(crawl_coronavirus)
View(crawl_coronavirus)
View(crawl_coronavirus)
function(){
con <- NULL
if(file.exists(config_file)){
con <- connect()
}
on.exit({
disconnect(con)
})
# get data
cli::cli_alert_info("Crawling data from John Hopkins")
# read data
suppressMessages({
confirmed <- readr::read_csv(confirmed_sheet, col_types = readr::cols())
#recovered <- readr::read_csv(recovered_sheet, col_types = readr::cols())
deaths <- readr::read_csv(deaths_sheet, col_types = readr::cols())
})
# add col
confirmed$type <- "confirmed"
#recovered$type <- "recovered"
deaths$type <- "death"
# rename
confirmed <- rename_sheets(confirmed)
#recovered <- rename_sheets(recovered)
deaths <- rename_sheets(deaths)
# pivot longer
confirmed <- pivot(confirmed)
#recovered <- pivot(recovered)
deaths <- pivot(deaths)
df <- dplyr::bind_rows(confirmed, deaths) %>%
dplyr::mutate(
date = as.Date(date, format = "%m/%d/%y"),
cases = trimws(cases),
cases = as.numeric(cases),
cases = dplyr::case_when(
is.na(cases) ~ 0,
TRUE ~ cases
),
country = dplyr::case_when(
country == "US" ~ "United States of America",
TRUE ~ country
),
country_iso2c = suppressWarnings(countrycode::countrycode(country, "country.name", "iso2c"))
)
cli::cli_alert_info("Crawling data from Weixin")
china <- nCov2019::get_nCov2019(lang = "zh")
china_daily <- china$chinaDayList %>%
dplyr::mutate(
date = paste0("2020.", date),
date = as.Date(date, "%Y.%m.%d")
) %>%
dplyr::mutate_if(is.character, as.numeric)
china_total <- as.data.frame(china$chinaTotal, stringsAsFactors = FALSE)
cli::cli_alert_info("Crawling data from DXY")
dxy_list <- xml2::read_html(dxy_url) %>%
rvest::html_node("#getAreaStat") %>%
rvest::html_text() %>%
gsub("try \\{ window.getAreaStat = ", "", .) %>%
gsub("\\}catch\\(e\\)\\{\\}", "", .) %>%
jsonlite::fromJSON()
dxy <- dxy_list %>%
dplyr::pull(cities) %>%
purrr::map2(dxy_list$provinceShortName, function(city, province){
if(nrow(city))
city$province <- province
return(city)
}) %>%
purrr::map2(dxy_list$provinceName, function(city, province){
if(nrow(city))
city$province_long <- province
return(city)
}) %>%
purrr::map_dfr(tibble::as_tibble) %>%
dplyr::left_join(chinese_provinces, by = c("province" = "chinese")) %>%
dplyr::rename(province_pinyin = state)
log <- tibble::tibble(last_updated = Sys.time())
# crawl news
news <- NULL
if(file.exists(config_file)){
if(has_newsapi()){
cli::cli_alert_info("Crawling news from newsapi.org")
set_news_api_token()
news <- newsapi::every_news("coronavirus OR covid", results = 100, language = "en", sort = "popularity")
} else {
cli::cli_alert_danger("Not `newsapi` entry in config file, not crawling news.")
}
}
# save
if(file.exists(config_file)){
cli::cli_alert_success("Writing to database")
DBI::dbWriteTable(con, "jhu", df, overwrite = TRUE, append = FALSE)
DBI::dbWriteTable(con, "weixin_total", china_total, overwrite = TRUE, append = FALSE)
DBI::dbWriteTable(con, "weixin", china_daily, overwrite = TRUE, append = FALSE)
DBI::dbWriteTable(con, "dxy", dxy, overwrite = TRUE, append = FALSE)
DBI::dbWriteTable(con, "log", log, append = TRUE)
DBI::dbWriteTable(con, "news", news, overwrite = TRUE)
}
dat <- list(
jhu = df,
weixin = china_daily,
weixin_total = china_total,
dxy = dxy,
news = news
)
invisible(dat)
}
# DXY LOCATIONS
dxy_url <- "https://ncov.dxy.cn/ncovh5/view/pneumonia"
source('~/myworkspace/AWSCorona/coronavirus-master/data-raw/internal.R')
# DXY LOCATIONS
dxy_url <- "https://ncov.dxy.cn/ncovh5/view/pneumonia"
dxy <- xml2::read_html(dxy_url) %>%
rvest::html_node("#getAreaStat") %>%
rvest::html_text() %>%
gsub("try \\{ window.getAreaStat = ", "", .) %>%
gsub("\\}catch\\(e\\)\\{\\}", "", .) %>%
jsonlite::fromJSON()
usethis::use_data(china_cities_location, chinese_provinces, default_province, china_population, internal = TRUE, overwrite = TRUE)
remotes::install_github("adityasinhak/covid19")
remotes::install_github("RinteRface/shinyMobile", force = TRUE)
library(coronavirus)
virus <- crawl_coronavirus()
run_app(virus)
